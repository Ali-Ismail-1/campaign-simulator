# ğŸ¤– Production-Grade RAG System

> A comprehensive Retrieval-Augmented Generation (RAG) implementation showcasing modern AI engineering practices, built for education and production deployment.

[![Next.js](https://img.shields.io/badge/Next.js-16.0-black?style=for-the-badge&logo=next.js)](https://nextjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue?style=for-the-badge&logo=typescript)](https://www.typescriptlang.org/)
[![LangChain](https://img.shields.io/badge/LangChain-1.0-green?style=for-the-badge)](https://langchain.com/)
[![Pinecone](https://img.shields.io/badge/Pinecone-Vector_DB-orange?style=for-the-badge)](https://www.pinecone.io/)

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Key Features](#-key-features)
- [Tech Stack](#-tech-stack)
- [RAG Pipeline Explained](#-rag-pipeline-explained)
- [Getting Started](#-getting-started)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [Security & Privacy](#-security--privacy)
- [Performance](#-performance)
- [Blog Series](#-blog-series)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Overview

This project demonstrates a **complete RAG (Retrieval-Augmented Generation) system** that combines vector search with large language models to provide accurate, context-aware answers from your own knowledge base.

**Built by an AI Engineer for AI Engineers** â€” This implementation prioritizes:
- ğŸ” **Transparency**: Every step is documented and explained
- ğŸ› ï¸ **Modularity**: Each component can be understood and modified independently
- ğŸ“ **Education**: Designed to teach RAG concepts through practical implementation
- ğŸ”’ **Security**: Built-in PII redaction and data sanitization
- ğŸ’° **Cost-Efficiency**: Local embeddings reduce API costs

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG PIPELINE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   User Question
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 1         â”‚
   â”‚  Embedding      â”‚â”€â”€â”€â”€ Xenova all-MiniLM-L6-v2 (384 dims)
   â”‚  Component      â”‚â”€â”€â”€â”€ Runs locally, no API calls
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Vector (float[384])
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 2         â”‚
   â”‚  Vector Store   â”‚â”€â”€â”€â”€ Pinecone Index
   â”‚  Connection     â”‚â”€â”€â”€â”€ Serverless, auto-scaling
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 3         â”‚
   â”‚  Semantic       â”‚â”€â”€â”€â”€ Cosine similarity search
   â”‚  Retrieval      â”‚â”€â”€â”€â”€ Top-K documents (k=4)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Matched Documents
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 4         â”‚
   â”‚  Context        â”‚â”€â”€â”€â”€ PII Redaction
   â”‚  Processing     â”‚â”€â”€â”€â”€ SSN, DOB, Salary filtering
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Sanitized Context
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 5         â”‚
   â”‚  LLM            â”‚â”€â”€â”€â”€ GPT-4o-mini
   â”‚  Generation     â”‚â”€â”€â”€â”€ Context + Question â†’ Answer
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
      Final Answer
```

---

## âœ¨ Key Features

### ğŸ§  **AI/ML Features**
- **Local Embeddings**: Privacy-first approach using Xenova Transformers
- **Vector Search**: Semantic similarity search powered by Pinecone
- **Context-Aware Generation**: LangChain orchestration with OpenAI GPT models
- **PII Protection**: Automatic redaction of sensitive information

### ğŸ› ï¸ **Engineering Features**
- **Type-Safe**: Full TypeScript implementation
- **Modular Design**: Clean separation of concerns
- **Error Handling**: Comprehensive error recovery
- **Environment-Based Config**: Easy deployment across environments
- **Modern Stack**: Next.js 16 with App Router

### ğŸ“š **Developer Experience**
- **Well-Documented**: Inline documentation explaining each step
- **Blog-Ready**: Code organized for educational content
- **Clean Architecture**: Easy to understand and extend
- **Open Source**: Learn from and contribute to the codebase

---

## ğŸ”§ Tech Stack

### **Frontend**
- **Next.js 16** - React framework with App Router
- **TypeScript** - Type safety and better DX
- **Tailwind CSS 4** - Modern, utility-first styling
- **React 19** - Latest React features

### **AI/ML**
- **LangChain** - LLM orchestration framework
- **OpenAI GPT-4o-mini** - Text generation
- **Xenova Transformers** - Local embedding model
- **Pinecone** - Vector database for semantic search

### **Infrastructure**
- **Vercel** - Deployment platform (recommended)
- **Pinecone Serverless** - Auto-scaling vector DB
- **Environment Variables** - Secure configuration management

---

## ğŸ”„ RAG Pipeline Explained

### What is RAG?

**Retrieval-Augmented Generation** enhances LLMs by providing relevant context from your knowledge base before generating answers. This solves several problems:

1. **Hallucination**: LLMs can make up information â†’ RAG grounds answers in real data
2. **Knowledge Cutoff**: LLMs have training data cutoffs â†’ RAG uses current information
3. **Domain-Specific**: LLMs lack specialized knowledge â†’ RAG provides your documents

### The Six Steps

```typescript
// STEP 1: Convert question to numbers (embeddings)
question â†’ [0.23, -0.45, 0.78, ...] // 384 dimensions

// STEP 2: Connect to vector database
Pinecone Index â† Pre-embedded documents

// STEP 3: Find similar documents
Query vector â†’ Search â†’ Top 4 matches

// STEP 4: Clean and prepare context
Retrieved text â†’ Remove PII â†’ Clean context

// STEP 5: Generate answer with LLM
Context + Question â†’ GPT-4o-mini â†’ Answer

// STEP 6: Return to user
Answer â†’ Display
```

---

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+ and npm
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))
- Pinecone account ([Sign up free](https://www.pinecone.io/))

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/rag-assistant.git
cd rag-assistant

# Install dependencies
npm install

# Set up environment variables
cp .env.example .env.local
# Edit .env.local with your API keys
```

### Environment Setup

Create a `.env.local` file:

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-api-key
OPENAI_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.7

# Pinecone Configuration
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX=your-index-name
```

### Create Pinecone Index

```bash
# Using Pinecone CLI or Dashboard
# Important: Set dimension to 384 (matches all-MiniLM-L6-v2)
Name: your-index-name
Dimensions: 384
Metric: cosine
```

### Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to see your RAG assistant!

---

## ğŸ“ Project Structure

```
rag-assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ chat/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts          # API endpoint for chat
â”‚   â”‚   â”œâ”€â”€ page.tsx                  # Main UI
â”‚   â”‚   â”œâ”€â”€ layout.tsx                # App layout
â”‚   â”‚   â””â”€â”€ globals.css               # Global styles
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â””â”€â”€ chain.ts              # â­ Core RAG pipeline (6 steps)
â”‚   â”‚   â”œâ”€â”€ pinecone/
â”‚   â”‚   â”‚   â””â”€â”€ client.ts             # Pinecone initialization
â”‚   â”‚   â””â”€â”€ langgraph/
â”‚   â”‚       â””â”€â”€ graph.ts              # Advanced orchestration
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ env.ts                    # Environment variable validation
â”œâ”€â”€ public/                           # Static assets
â”œâ”€â”€ .env.local                        # Environment variables (create this)
â”œâ”€â”€ .env.example                      # Environment template
â”œâ”€â”€ package.json                      # Dependencies
â”œâ”€â”€ tsconfig.json                     # TypeScript configuration
â””â”€â”€ README.md                         # This file
```

---

## âš™ï¸ Configuration

### Embedding Models

**Current**: `xenova/all-MiniLM-L6-v2` (384 dimensions, local)

**Why Local Embeddings?**
- âœ… **Privacy**: Questions never leave your server
- âœ… **Cost**: Completely free, no API charges
- âœ… **Speed**: After initial download (~50MB), it's faster
- âœ… **Compliance**: GDPR/HIPAA friendly

**Alternatives**:
```typescript
// OpenAI Small (cloud, 1536 dims)
const embeddings = new OpenAIEmbeddings({
    model: 'text-embedding-3-small'
});

// OpenAI Large (cloud, 3072 dims)
const embeddings = new OpenAIEmbeddings({
    model: 'text-embedding-3-large'
});
```

âš ï¸ **Important**: Pinecone index dimensions must match your embedding model!

### LLM Models

**Current**: `gpt-4o-mini` (Fast, cost-effective)

**Alternatives**:
- `gpt-4o` - More capable, higher cost (~15x)
- `gpt-4-turbo` - Balanced performance
- `gpt-3.5-turbo` - Faster, lower cost (legacy)

### Retrieval Configuration

```typescript
// In src/lib/rag/chain.ts

// Number of documents to retrieve
const topK = 4; // Adjust based on your needs (2-10 typical)

// Similarity threshold (optional)
scoreThreshold: 0.7 // Only return matches above this score
```

---

## ğŸ”’ Security & Privacy

### PII Redaction

The system automatically redacts sensitive information before sending to the LLM:

- **Social Security Numbers**: `123-45-6789` â†’ `SSN: [REDACTED]`
- **Date of Birth**: `01/15/1990` â†’ `DOB: [REDACTED]`
- **Salary Information**: `$125,000` â†’ `Salary: [REDACTED]`

```typescript
// Customize in src/lib/rag/chain.ts
function sanitizeForLLM(text: string, options = {
    removeSSN: true,
    removeDOB: true,
    removeSalary: true
})
```

**Add Custom Redaction:**
```typescript
// Example: Redact credit cards
if (options.removeCreditCard) {
    cleaned = cleaned.replace(
        /\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}/g,
        'CC: [REDACTED]'
    );
}
```

### Local Embeddings

Using Xenova means:
- âœ… Questions never sent to embedding API
- âœ… No external embedding costs
- âœ… Privacy-compliant processing
- âœ… Works offline (after model download)

### API Security

**Recommended Additions** (not implemented yet):
- Rate limiting (e.g., `next-rate-limit`)
- API key authentication
- CORS configuration
- Request size limits

---

## âš¡ Performance

### Benchmarks

| Operation | Time | Notes |
|-----------|------|-------|
| **Cold Start** | ~2-3s | First query, model loading |
| **Warm Queries** | ~1-2s | Subsequent queries |
| **Embedding** | ~100-200ms | Local processing |
| **Vector Search** | ~50-100ms | Pinecone latency |
| **LLM Generation** | ~1-2s | OpenAI API call |

### Optimization Tips

```typescript
// 1. Reduce retrieved documents (faster, less context)
const results = await retrieveDocuments(queryEmbedding, 2); // Instead of 4

// 2. Lower temperature (faster, more deterministic)
temperature: 0.3 // Instead of 0.7

// 3. Use smaller model
model: 'gpt-4o-mini' // Already optimized!

// 4. Implement caching
// Cache frequent questions to avoid repeat processing
```

### Scalability Considerations

- **Pinecone**: Serverless tier auto-scales
- **Next.js**: Deploy on Vercel for edge caching
- **Embeddings**: Model loads once per serverless function
- **Rate Limiting**: Implement to control costs

---

## ğŸ“ Blog Series

This codebase is designed to support a comprehensive blog series on RAG implementation:

### Planned Articles

1. **Introduction to RAG** - What, why, and when to use RAG
2. **Vector Embeddings Deep Dive** - Understanding the math behind semantic search
3. **Vector Databases Explained** - Why Pinecone, and how it works
4. **Semantic Search Mechanics** - Cosine similarity and retrieval strategies
5. **Context Processing & Security** - PII redaction and data sanitization
6. **LLM Prompt Engineering** - Crafting effective prompts for RAG
7. **Production Considerations** - Security, scaling, and monitoring
8. **Advanced RAG Patterns** - Hybrid search, re-ranking, and more

> **Coming Soon**: Links will be added as articles are published

### Use This Code

Each component in `src/lib/rag/chain.ts` is documented as a standalone step, making it easy to explain in blog posts or tutorials. The code is designed to be:

- **Copy-paste friendly** - Each step works independently
- **Clearly commented** - Explanations inline with code
- **Production-quality** - Not just toy examples

---

## ğŸ¤ Contributing

Contributions are welcome! This project serves as both a production implementation and educational resource.

### Areas for Contribution

- ğŸ› Bug fixes and error handling improvements
- ğŸ“š Documentation enhancements
- ğŸ¨ UI/UX improvements
- ğŸ§ª Test coverage
- ğŸ”§ New features (streaming, caching, authentication)
- ğŸ“– Blog content and tutorials
- ğŸŒ Internationalization

### Development Guidelines

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Standards

- TypeScript for type safety
- ESLint for code quality
- Meaningful commit messages
- Documentation for new features

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

**TL;DR**: You can use this code for anything, including commercial projects. Just keep the copyright notice.

---

## ğŸ“ About the Author

**AI Engineer | Developer Advocate**

This project demonstrates:
- âœ… Production-grade AI system design
- âœ… Clear technical communication
- âœ… Modern full-stack development
- âœ… Educational content creation
- âœ… Open-source contribution

Building in public and sharing knowledge about AI engineering. Passionate about making complex AI concepts accessible and practical.

### Why This Project?

RAG systems are becoming fundamental to AI applications, but many tutorials skip important details like:
- How to handle PII in retrieved context
- Why certain embedding models work better
- The tradeoffs between local and cloud processing
- How to structure code for production

This project addresses those gaps.

**Connect with me**:
- ğŸ¦ Twitter: [@Ali_F_Ismail]
- ğŸ’¼ LinkedIn: [https://www.linkedin.com/in/ali-ismail-35196615/]
- ğŸ“ Blog: [https://ali-ismail.com/]
- ğŸ™ GitHub: [@ali-ismail-1]

---

## ğŸ™ Acknowledgments

- [LangChain](https://langchain.com/) - For the excellent LLM orchestration framework
- [Pinecone](https://www.pinecone.io/) - For the vector database infrastructure
- [Xenova](https://github.com/xenova/transformers.js) - For bringing transformers to JavaScript
- [Next.js](https://nextjs.org/) - For the incredible React framework
- [OpenAI](https://openai.com/) - For the GPT models

---

## ğŸ“š Additional Resources

### Learning RAG
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [Pinecone Learning Center](https://www.pinecone.io/learn/)
- [Vector Embeddings Explained](https://www.youtube.com/watch?v=wjZofJX0v4M)

### Related Projects
- [LangChain.js](https://github.com/langchain-ai/langchainjs)
- [Transformers.js](https://github.com/xenova/transformers.js)
- [Vercel AI SDK](https://sdk.vercel.ai/docs)

---

<div align="center">

**â­ Star this repo if you find it helpful!**

[Report Bug](https://github.com/yourusername/rag-assistant/issues) Â· [Request Feature](https://github.com/yourusername/rag-assistant/issues) Â· [Documentation](https://github.com/yourusername/rag-assistant/wiki)

Made with â¤ï¸ and â˜• by an AI Engineer

</div>
